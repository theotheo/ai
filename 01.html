<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><title>Некоторая история искусственного интеллекта</title><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui" name="viewport"><link href="reveal.js/css/reveal.css" rel="stylesheet"><link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme"><!--This CSS is generated by the Asciidoctor-Reveal.js converter to further integrate AsciiDoc's existing semantic with Reveal.js--><style type="text/css">.reveal div.right {
  float: right;
}

/* callouts */
.conum[data-value] {display:inline-block;color:#fff!important;background-color:rgba(50,150,50,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]:after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}</style><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
tex2jax: {
  inlineMath: [["\\(", "\\)"]],
  displayMath: [["\\[", "\\]"]],
  ignoreClass: "nostem|nolatexmath"
},
asciimath2jax: {
  delimiters: [["\\$", "\\$"]],
  ignoreClass: "nostem|noasciimath"
},
TeX: { equationNumbers: { autoNumber: "none" } }
});</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.4.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script><link href="reveal.js/lib/css/zenburn.css" rel="stylesheet"><script>var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? "reveal.js/css/print/pdf.css" : "reveal.js/css/print/paper.css";
document.getElementsByTagName( 'head' )[0].appendChild( link );</script><!--[if lt IE 9]><script src="reveal.js/lib/js/html5shiv.js"></script><![endif]--><link rel="stylesheet" href="asciidoctor-revealjs.css"></head><body><div class="reveal"><div class="slides"><section class="title" data-state="title"><h1>Некоторая история искусственного интеллекта</h1></section><section><section id="_обо_мне"><h2>Обо мне</h2><div class="paragraph"><p>Белялов Ильдар</p></div></section><section id="_инженер_программист"><h2>Инженер-программист</h2><div class="imageblock" style=""><img src="images/mirea.jpg" alt="mirea"></div></section><section id="_rd_инженер_недавнее_место_работы"><h2>R&amp;D-инженер (недавнее место работы)</h2><div class="imageblock" style=""><img src="images/epoch8-back.png" alt="epoch8 back"></div></section><section id="_организатор_мастерской_deep_learning"><h2>Организатор мастерской Deep Learning</h2></section><section id="_лш_18"><h2>ЛШ 18</h2><div class="imageblock" style=""><img src="images/lsh18.jpg" alt="lsh18"></div></section><section id="_лш_19"><h2>ЛШ 19</h2><div class="imageblock" style=""><img src="images/lsh19.jpg" alt="lsh19"></div>
<div class="paragraph"><p><a href="https://www.facebook.com/commanderduck/videos/10221014010799488" class="bare">https://www.facebook.com/commanderduck/videos/10221014010799488</a></p></div></section><section id="_научный_руководитель"><h2>Научный руководитель</h2><div class="imageblock" style=""><img src="images/vkr1.jpg" alt="vkr1"></div></section><section id="_контакты"><h2>Контакты</h2><div class="admonitionblock note"><table><tr><td class="icon"><i class="fa fa-info-circle" title="Note"></i></td><td class="content"><div class="paragraph"><p>Обращайтесь!</p></div>
<div class="ulist"><ul><li><p><a href="http://t.me/ibelyalov" class="bare">http://t.me/ibelyalov</a></p></li><li><p><a href="http://vk.me/theotheo" class="bare">http://vk.me/theotheo</a></p></li></ul></div></td></tr></table></div></section><section id="_о_курсе"><h2>О курсе</h2><div class="ulist"><ul><li class="fragment"><p>Провожу курс первый раз</p></li><li class="fragment"><p>Материалы сырые&#8201;&#8212;&#8201;будут ошибки</p></li><li class="fragment"><p>Вопросы&#8201;&#8212;&#8201;это прекрасно!</p></li><li class="fragment"><p>Если не поняли, то проблема в лекторе</p></li><li class="fragment"><p>Шлите вопрос&#8201;&#8212;&#8201;обязательно разберем в процессе</p></li><li class="fragment"><p>«Повторение — мать учения»</p></li><li class="fragment"><p>Вещь, повторенная дважды запоминается лучше. Вещь, повторенная дважды запоминается лучше.</p></li><li class="fragment"><p>«Repetitio est mater studiorum»</p></li></ul></div></section></section>
<section id="_докомпьютерная_эра"><h2>Докомпьютерная эра</h2><section id="_98_вв_до_н_э_илиада"><h2>9—8 вв. до н. э., Илиада</h2><div class="ulist"><ul><li><p>Гомер</p></li><li><p>древнегреческая мифология</p></li><li><p>Гефест, бог огня и кузнец</p></li><li><p>создал из металла помощников для кузницы</p></li></ul></div></section>
<section id="_где_то_тогда_же_талос"><h2>где-то тогда же, Талос</h2><div class="ulist"><ul><li><p>древнегреческая мифология</p></li><li><p>бронзовый витязь, данный Зевсом Европе для охраны острова Крит</p></li><li><p>три раза в день он обегал весь остров</p></li><li><p>протяжённость береговой линии — 1066 километром</p></li></ul></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Crete_topographic_map-ru.svg/280px-Crete_topographic_map-ru.svg.png" alt="280px Crete topographic map ru.svg"></div></section>
<section id="_5_век_до_н_э_органон"><h2>5 век до н.э. "Органон"</h2><div class="ulist"><ul><li><p>(др.-греч. Ὄργανον — инструмент, метод)</p></li><li><p>Аристотель (384 год до н. э. - 322 год до н. э.)</p></li><li><p>Силлогистическая логика&#8201;&#8212;&#8201;первая формальная система дедуктивной логики</p></li></ul></div>
<div class="ulist"><div class="title">Содержание</div><ul><li><p>Категории (греч. Κατηγορίαι; лат. Categoriae, или Praedicamenta)</p></li><li><p>Об истолковании (греч. Περὶ ἑρμηνείας; лат. De interpretatione)</p></li><li><p>Первая аналитика (греч. Ἀναλυτικὰ πρότερα; лат. Analytica priora)</p></li><li><p>Вторая аналитика (греч. Ἀναλυτικὰ ὕστερα; лат. Analytica posteriora)</p></li><li><p>Топика (греч. Τοπικά; лат. Topica)</p></li><li><p>Софистические опровержения (греч. Περὶ τῶν σοφιστικῶν ἐλέγχων, букв. О софистических опровержениях; лат. Sophistici elenchi)</p></li></ul></div></section>
<section id="_дедукция_и_индукция"><h2>Дедукция и индукция</h2><div class="ulist"><ul><li><p>Деду́кция (лат. deductio — выведение[1], также дедуктивное умозаключение, силлогизм[2]) — метод мышления, следствием которого является логический вывод, в котором частное заключение выводится из общего.</p></li><li><p>Инду́кция (лат. inductio — наведение, от лат. inducere — влечь за собой, установить) — процесс логического вывода на основе перехода от частного положения к общему[1]. Индуктивное умозаключение связывает частные предпосылки с заключением не строго через законы логики, а скорее через некоторые фактические, психологические или математические представления</p></li></ul></div></section>
<section id="_дедукция_и_индукция_2"><h2>Дедукция и индукция</h2><div class="paragraph"><p>Пример дедукции:</p></div>
<div class="olist arabic"><ol class="arabic"><li><p>Все люди смертны.</p></li><li><p>Сократ — человек.</p></li><li><p>Следовательно, Сократ смертен.</p></li></ol></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Induktion-Deduktion-ru.svg/1280px-Induktion-Deduktion-ru.svg.png" alt="1280px Induktion Deduktion ru.svg"></div></section>
<section id="_10_век_сильвестр_ii"><h2>10 век. Сильве́стр II</h2><div class="ulist"><ul><li><p>ок. 946 — 12 мая 1003</p></li><li><p>средневековый учёный и церковный деятель</p></li><li><p>изучал математику и астрономию</p></li><li><p>возродил использование абака</p><div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/b/b5/RomanAbacusRecon.jpg" alt="RomanAbacusRecon"></div></li><li><p>астролябии</p><div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/c/cb/Planispheric_astrolabe.png" alt="Planispheric astrolabe"></div></li></ul></div></section>
<section id="_13_век_раймунд_луллий"><h2>13 век. Райму́нд Лу́ллий</h2><div class="ulist"><ul><li><p>a Spanish theologian</p></li><li><p>invents machines for discovering non-mathematical truths using combinatorics.</p></li></ul></div></section>
<section id="_1620_новый_органон"><h2>1620, "Новый органон"</h2><div class="ulist"><ul><li><p>Фрэнсис Бекон (1561-1626)</p></li><li><p>был предназначен для замены методов, которые были предложены в сочинении «Органон»</p></li><li><p>в основе научного познания должны лежать индукция и эксперимент (data science?!)</p></li><li><p>истинное знание вытекает из чувственного опыта</p></li><li><p>"эмпиризм"</p></li></ul></div>
<div class="quoteblock"><blockquote>Бэкон планировал изложить свою основную философскую идею — преобразование наук с целью подчинить природу могуществу человека — в громадном сочинении под заглавием «Великое возрождение наук» (Instauratio Magna), которое должно было состоять из шести частей. Средством для достижения преобразования наук предлагались наблюдение и опыт, то есть индуктивный метод.</blockquote><div class="attribution">&#8212; <a href="https://ru.wikipedia.org/wiki/Новый_органон" class="bare">https://ru.wikipedia.org/wiki/Новый_органон</a></div></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Novum_Organum_1650_crop.jpg/250px-Novum_Organum_1650_crop.jpg" alt="250px Novum Organum 1650 crop"></div></section>
<section id="_1637_рассуждение_о_методе_чтобы_хорошо_направлять_свой_разум_и_отыскивать_истину_в_науках"><h2>1637, "Рассуждение о методе, чтобы хорошо направлять свой разум и отыскивать истину в науках"</h2><div class="ulist"><ul><li><p>Рене Декарт (1596-1650)</p></li><li><p>cogito ergo sum (Je pense, donc je suis)</p></li><li><p>Самодостоверность сознания, cogito (декартовское «мыслю, следовательно, существую» — лат. Cogito, ergo sum), равно как и теория врождённых идей, является исходным пунктом картезианской гносеологии.</p></li><li><p>создатель аналитической геометрии и современной алгебраической символики</p></li></ul></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/3/3f/Descartes_Discours_de_la_Methode.jpg" alt="Descartes Discours de la Methode"></div></section>
<section id="_1690_опыт_о_человеческом_разумении"><h2>1690, Опыт о человеческом разумении</h2><div class="ulist"><ul><li><p>Джон Локк (1632-1704)</p></li><li><p>Tabula rasa (чистая доска)</p></li></ul></div>
<div class="quoteblock"><blockquote>Предположим, что ум есть, так сказать, белая бумага без всяких знаков и идей. Но каким же образом он получает их? Откуда он приобретает тот [их] обширный запас, который деятельное и беспредельное человеческое воображение нарисовало с почти бесконечным разнообразием? Откуда получает он весь материал рассуждения и знания? На это я отвечаю одним словом: из опыта</blockquote><div class="attribution">&#8212; <a href="https://ru.wikipedia.org/wiki/Tabula_rasa" class="bare">https://ru.wikipedia.org/wiki/Tabula_rasa</a></div></div></section>
<section id="_1781_критика_чистого_разума"><h2>1781, Критика чистого разума</h2><div class="ulist"><ul><li><p>Иманнуил Кант (1724-1804)</p></li></ul></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/b/b9/Kant-KdrV-1781.png" alt="Kant KdrV 1781"></div></section>
<section id="_1796_механический_турок"><h2>1796, механический турок</h2><div class="quoteblock"><blockquote>Первый шахматный автомат сконструирован Вольфгангом фон Кемпеленом и продемонстрирован в Вене в 1769 году. Он был в виде «турка» - восковой фигуры человека в натуральную величину, одетого в турецкий наряд, сидящего за шахматной доской, которая стояла на деревянном ящике (размером 1,2×0,6×0,9 м). В ящике были дверцы, которые раскрывались и публике демонстрировался сложный механизм с различными узлами и деталями. Потом дверцы закрывались, механизм заводился ключом и начиналась игра, которую вёл сильный шахматист, который сидел в ящике и был скрыт системой зеркал и перегородок</blockquote><div class="attribution">&#8212; <a href="https://ru.wikipedia.org/wiki/Шахматный_автомат" class="bare">https://ru.wikipedia.org/wiki/Шахматный_автомат</a></div></div>
<div class="ulist"><ul><li><p>в 1834 году Муре раскрыл секрет автомата, опубликовав о нём статью в парижском журнале «Магазин питтореск»</p></li><li><p>1836 — в Америке писатель Эдгар По опубликовал большую статью под заглавием «Игрок в шахматы Мельзеля»</p></li><li><p>В 1850 году в Лондоне вышел сборник с описанием 50 партий, сыгранных за автомат французским шахматистом Ж. Муре.</p></li></ul></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/2/25/Turk-engraving5.jpg" alt="Turk engraving5"></div></section>
<section id="_1818_чудовище_франкенштейна"><h2>1818, Чудовище Франкенштейна</h2><div class="ulist"><ul><li><p>Мэри Шелли (1797-1851)</p></li><li><p>Франкенштейн, или Современный Прометей</p></li><li><p>"В романе Виктор Франкенштейн хочет создать живое существо из неживой материи."</p></li></ul></div></section>
<section id="_1854_laws_of_thought"><h2>1854, Laws of Thought</h2><div class="ulist"><ul><li><p>Джордж Буль</p></li><li><p>Boole systematically presented logic as a system of formal rules which turned out to be a major milestone in the reshaping of logic as a formal science. Quickly after, formal logic developed, and today it is considered a native branch of both philosophy and mathematics, with abundant applications to computer science</p></li></ul></div></section>
<section id="_1910_1913_principia_mathematica"><h2>1910-1913, Principia Mathematica</h2><div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Russell%2C_Whitehead_-_Principia_Mathematica_to_56.jpg/255px-Russell%2C_Whitehead_-_Principia_Mathematica_to_56.jpg" alt="255px Russell%2C Whitehead   Principia Mathematica to 56"></div>
<div class="ulist"><ul><li><p>2000 стр</p></li><li><p>"стремились показать, что вся математика сводится к логике с помощью набора аксиом и нескольких основных понятий"</p></li></ul></div></section>
<section id="_1930_теорема_гёделя_о_неполноте_вторая_теорема_гёделя"><h2>1930, Теорема Гёделя о неполноте вторая теорема Гёделя</h2><div class="ulist"><ul><li><p>Курт Гедель, 1906-1978, австрийский логик, математик и философ математики</p></li><li><p>две теоремы математической логики о принципиальных ограничениях формальной арифметики и, как следствие, всякой формальной системы, в которой можно определить основные арифметические понятия: натуральные числа, 0, 1, сложение и умножение.</p></li><li><p>показывает слабости формальных систем</p></li></ul></div></section></section>
<section id="_зарождение"><h2>Зарождение</h2><section id="_1943_a_logical_calculus_of_ideas_immanent_in_nervous_activity"><h2>1943, A Logical Calculus of Ideas Immanent in Nervous Activity</h2><div class="ulist"><ul><li><p>Мак-Каллок (нейрофизиолог), Питс (математик)</p></li><li><p>on how neurons might work.</p></li><li><p>modeled a simple neural network using electrical circuits.</p></li><li><p>The paper had only three references, and all of them are classical works in logic: Carnap’s Logical Syntax of Language [6], Russell’s and Whitehead’s Principa Mathematica [7] and the Hilbert and Ackermann Grundüge der Theoretischen Logik.</p></li><li><p>The paper itself approached the problem of neural networks as a logical one, proceeding from definitions, over lemmas to theorems.</p></li><li><p>Питтс в 12 лет прочел "Принципы математики" и написал Расселу</p></li><li><p>Мак-Каллок работал с Винером</p></li></ul></div></section>
<section id="_1948_cybernetics_or_control_and_communication_in_the_animal_and_the_machine"><h2>1948, Cybernetics: Or Control and Communication in the Animal and the Machine</h2><div class="ulist"><ul><li><p>It is the first public usage of the term "cybernetics" to refer to self-regulating mechanisms.</p></li><li><p>The book laid the theoretical foundation for servomechanisms (whether electrical, mechanical or hydraulic), automatic navigation, analog computing, artificial intelligence, neuroscience, and reliable communications.</p></li></ul></div>
<div class="paragraph"><p>Содержание (второго издания)</p></div>
<div class="olist arabic"><ol class="arabic"><li><p>Newtonian and Bergsonian Time</p></li><li><p>Groups and Statistical Mechanics</p></li><li><p>Time Series, Information, and Communication</p></li><li><p>Feedback and Oscillation</p></li><li><p>Computing Machines and the Nervous System</p></li><li><p>Gestalt and Universals</p></li><li><p>Cybernetics and Psychopathology</p></li><li><p>Information, Language, and Society</p></li><li><p>On Learning and Self-Reproducing Machines</p></li><li><p>Brain Waves and Self-Organising Systems</p></li></ol></div></section>
<section id="_1949_hebb_rule"><h2>1949 Hebb Rule</h2><div class="ulist"><ul><li><p>Donald Hebb</p></li><li><p>The Organization of Behavior</p></li><li><p>pointed out the fact that neural pathways are strengthened each time they are used, a concept fundamentally essential to the ways in which humans learn. If two nerves fire at the same time, he argued, the connection between them is enhanced.</p></li></ul></div></section>
<section id="_1949_клуб_ratio"><h2>1949, клуб Ratio</h2><div class="ulist"><ul><li><p>1949 to 1958</p></li><li><p>small British informal dining club</p></li><li><p>young psychiatrists, psychologists, physiologists, mathematicians and engineers who met to discuss issues in cybernetics</p></li><li><p>The initial membership was W. Ross Ashby, Horace Barlow, John Bates, George Dawson, Thomas Gold, W. E. Hick, Victor Little, Donald MacKay, Turner McLardy, P. A. Merton, John Pringle, Harold Shipton, Donald Sholl, Eliot Slater, Albert Uttley, W. Grey Walter and John Hugh Westcott. Alan Turing joined after the first meeting with I. J. Good, Philip Woodward and William Rushton added soon after. Giles Brindley was also a member for a short period.</p></li><li><p>The club was the most intellectually powerful and influential cybernetics grouping in the UK, and many of its members went on to become extremely prominent scientists.</p></li></ul></div></section>
<section id="_1950_тест_тьюринга"><h2>1950, Тест Тьюринга</h2><div class="ulist"><ul><li><p>Алан Тьюринг</p></li><li><p>Computing Machinery and Intelligence</p></li><li><p>"I propose to consider the question 'Can machines think?'"</p></li></ul></div>
<div class="imageblock" style=""><img src="https://en.wikipedia.org/wiki/File:Turing_test_diagram.png" alt="File:Turing test diagram"></div></section>
<section id="_1950_the_unreasonable_effectiveness_of_mathematics_in_the_natural_sciences"><h2>1950, The Unreasonable Effectiveness of Mathematics in the Natural Sciences</h2><div class="ulist"><ul><li><p>Eugene Wigner (1902-1995)</p></li></ul></div>
<div class="paragraph"><p><a href="https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences" class="bare">https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences</a></p></div></section>
<section id="_1954_ibm_704"><h2>1954, IBM 704</h2><div class="ulist"><ul><li><p>the first mass-produced computer with floating-point arithmetic hardware</p></li><li><p>IBM sold 123 type 704 systems between 1955 and 1960</p></li></ul></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/IBM_Electronic_Data_Processing_Machine_-_GPN-2000-001881.jpg/420px-IBM_Electronic_Data_Processing_Machine_-_GPN-2000-001881.jpg" alt="420px IBM Electronic Data Processing Machine   GPN 2000 001881"></div></section>
<section id="_1954_cold_war"><h2>1954, Cold war</h2></section>
<section id="_1955_logic_theorist"><h2>1955, Logic Theorist</h2><div class="ulist"><ul><li><p>RAND Corporation</p></li><li><p>Аллен Ньюэлл, Герберт Саймон and Cliff Shaw</p></li><li><p>the first program deliberately engineered to mimic the problem solving skills of a human being</p></li><li><p>"the first artificial intelligence program"</p></li><li><p>prove 38 of the first 52 theorems in Principia Mathematica</p></li><li><p>одно из доказательств было даже более элегантным</p></li></ul></div></section>
<section id="_1956_дартмутский_семинар"><h2>1956, Дартмутский семинар</h2><div class="ulist"><ul><li><p>Дартмутской колледж</p></li><li><p>двухмесячный научный семинар по вопросам искусственного интеллекта</p></li><li><p>John McCarthy, Marvin Minsky, Julian Bigelow, Donald MacKay, Ray Solomonoff,
John Holland, Claude Shannon, Nathanial Rochester, Oliver Selfridge, Allen Newell and Herbert Simon</p></li></ul></div>
<div class="quoteblock"><blockquote>Мы предлагаем исследование искусственного интеллекта сроком в 2 месяца с участием 10 человек летом 1956 года в Дартмутском колледже, Гановер, Нью-Гемпшир. Исследование основано на предположении, что всякий аспект обучения или любое другое свойство интеллекта может в принципе быть столь точно описано, что машина сможет его симулировать. Мы попытаемся понять, как обучить машины использовать естественные языки, формировать абстракции и концепции, решать задачи, сейчас подвластные только людям, и улучшать самих себя. Мы считаем, что существенное продвижение в одной или более из этих проблем вполне возможно, если специально подобранная группа учёных будет работать над этим в течение лета</blockquote><div class="attribution">&#8212; <a href="https://ru.wikipedia.org/wiki/Дартмутский_семинар" class="bare">https://ru.wikipedia.org/wiki/Дартмутский_семинар</a></div></div></section>
<section id="_1956_artificial_intelligence"><h2>1956, Artificial intelligence</h2><div class="ulist"><ul><li><p>Появление термина</p></li></ul></div></section>
<section id="_1958_lisp"><h2>1958 LISP</h2><div class="ulist"><ul><li><p>"LISt Processor"</p></li><li><p>pioneered many ideas in computer science, including tree data structures, automatic storage management, dynamic typing, conditionals, higher-order functions, recursion, the self-hosting compiler, and the read–eval–print loop</p></li></ul></div>
<div class="imageblock" style=""><img src="images/2019-09-13-12-23-58.png" alt="2019 09 13 12 23 58"></div></section>
<section id="_1959_general_problem_solver"><h2>1959 General Problem Solver</h2><div class="ulist"><ul><li><p>более мощного инструмента, чем Logical Theorist</p></li><li><p>программа могла не только доказывать утверждения, но и играть в шахматы и ханойские башни.</p></li><li><p>Программа раскладывала проблему на более простые составляющие, решение которых возможно достичь.</p></li><li><p>1972 «Решение проблем человеком», Ньюэлл и Саймон обобщили результаты этих исследований, а также рассказали об исследованиях, объектами которых были люди, решавшие математические и логические головоломки</p></li><li><p>it could not solve any real-world problems because search was easily lost in the combinatorial explosion.</p></li><li><p>демонстрировал результаты, которые не могли нейросети</p></li><li><p>доказательство теорем считалось чуть ли не вершиной интеллекта, в отличии от распознавания образов</p></li></ul></div></section>
<section id="_1958_perceptron"><h2>1958 Perceptron</h2><div class="ulist"><ul><li><p>Cornell Aeronautical Laboratory</p></li><li><p>Фрэнк Розенблатт, 1928-1971,  американский учёный в области психологии, нейрофизиологии</p></li><li><p>первоначально в 1957 как программа для  IBM 704</p></li><li><p>первый "нейрокомпьютер"</p></li><li><p>1962 - книга Principles of Neurodynamic</p></li></ul></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/ru/8/8b/Rosenblatt.jpg" alt="Rosenblatt"></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg" alt="Mark I perceptron"></div></section>
<section id="_1959_adalinemadaline"><h2>1959 ADALINE/MADALINE</h2><div class="ulist"><ul><li><p>Bernard Widrow and Marcian Hoff</p></li><li><p>ADALINE (ADAptive LInear NEuron), MADALINE (Many/Multiple ADALINE)</p></li><li><p>It is based on the McCulloch–Pitts neuron.</p></li><li><p>ADALINE was developed to recognize binary patterns so that if it was reading streaming bits from a phone line, it could predict the next bit.</p></li><li><p>нейрокомпьютер</p></li><li><p>MADALINE was the first neural network applied to a real world problem, using an adaptive filter that eliminates echoes on phone lines. While the system is as ancient as air traffic control systems, like air traffic control systems, it is still in commercial use.</p></li></ul></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/be/Adaline_flow_chart.gif/375px-Adaline_flow_chart.gif" alt="375px Adaline flow chart"></div>
<div class="imageblock" style=""><img src="http://scask.ru/archive/arch.php?path=../htm/stu.scask/book_ns/files.book&file=ns_10.files/image2.gif" alt="image2"></div></section>
<section id="_1959_verbal_behavior"><h2>1959, Verbal Behavior</h2><div class="ulist"><ul><li><p>Noam Chomsky (1928-)</p></li><li><p>"The fact that all normal children acquire essentially comparable grammars of great complexity with remarkable rapidity suggests that human beings are somehow specially designed to do this, with data-handling or "hypothesis-formulating" ability of unknown character and complexity."</p></li><li><p>долой бихевиоризм</p></li><li><p>когнитивная революция</p></li><li><p>cognitive science: anthropology, computer science, linguistic, neuroscience, philosophy and psychology.</p></li></ul></div></section>
<section id="_1962_widrow_hoff_developed_a_learning_procedure"><h2>1962, Widrow &amp; Hoff developed a learning procedure</h2><div class="paragraph"><p>examines the value before the weight adjusts it (i.e. 0 or 1) according to the rule: Weight Change = (Pre-Weight line value) * (Error / (Number of Inputs)). It is based on the idea that while one active perceptron may have a big error, one can adjust the weight values to distribute it across the network, or at least to adjacent perceptrons. Applying this rule still results in an error if the line before the weight is 0, although this will eventually correct itself. If the error is conserved so that all of it is distributed to all of the weights than the error is eliminated.</p></div></section>
<section id="_1963_sail"><h2>1963, SAIL</h2><div class="ulist"><ul><li><p>Stanford Artificial Intelligence Laboratory</p></li><li><p>Джон Маккарти сооснователь</p></li><li><p>The current director is Professor Chris Manning</p></li></ul></div></section>
<section id="_1965_dendral"><h2>1965-, Dendral</h2><div class="ulist"><ul><li><p>"Dendritic Algorithm"</p></li><li><p>Stanford</p></li><li><p>Edward Feigenbaum, Bruce G. Buchanan, Joshua Lederberg, and Carl Djerassi, along with a team of highly creative research associates and students</p></li><li><p>Its primary aim was to study hypothesis formation and discovery in science.</p></li><li><p>идентификации органических соединений с помощью анализа масс-спектрограмм.</p></li><li><p>LISP</p></li><li><p>многие производные системы, including MYCIN, MOLGEN, PROSPECTOR, XCON, and STEAMER.</p></li><li><p>написано более 20 научных работ по результатам работы системы DENDRAL с реальными задачами.</p></li><li><p>сейчас существуют много программ для той же задачи, но их не называют ИИ</p></li></ul></div></section>
<section id="_1966_eliza"><h2>1966, Eliza</h2><div class="ulist"><ul><li><p>Joseph Weizenbaum</p></li><li><p>MIT AI Lab</p></li><li><p>pattern matching</p></li><li><p>роджерианский терапевт</p></li><li><p>один из первых чатботов
image::https://upload.wikimedia.org/wikipedia/commons/4/4e/ELIZA_conversation.jpg[]</p></li></ul></div></section>
<section id="_1966_failure_of_machine_translation"><h2>1966, failure of machine translation</h2><div class="ulist"><ul><li><p>‘the spirit was willing but theflesh was weak’</p></li><li><p>‘the vodka was good, but the meat was rotten’.</p></li></ul></div></section>
<section id="_1969_perceptrons"><h2>1969, Perceptrons</h2><div class="ulist"><ul><li><p>Perceptrons: an introduction to computational geometry</p></li><li><p>Marvin Minsky and Seymour Papert</p></li><li><p>XOR-problem</p></li></ul></div></section>
<section id="_1973_lighthill_report"><h2>1973, Lighthill report</h2><div class="ulist"><ul><li><p>James Lighthill (1924-1988), British applied mathematician</p></li><li><p>Artificial Intelligence: A General Survey</p></li><li><p>British Science Research Council</p></li><li><p>закрыл все, кроме 3 департаментов: Edinburgh, Sussex and Essex</p></li></ul></div></section>
<section id="_1975_backpropagation"><h2>1975, backpropagation</h2><div class="ulist"><ul><li><p>Paul Werbos, an economist by degree, discovered backpropagation, a way to propagate the error back through the hidden (middle) layer</p></li></ul></div></section>
<section id="_1974_1980_ai_winter"><h2>1974-1980 AI Winter</h2><div class="quoteblock"><blockquote>Despite the later success of the neural network, traditional von Neumann architecture took over the computing scene, and neural research was left behind. Ironically, John von Neumann himself suggested the imitation of neural functions by using telegraph relays or vacuum tubes.</blockquote><div class="attribution">&#8212; quote</div></div>
<div class="quoteblock"><blockquote>In the same time period, a paper was written that suggested there could not be an extension from the single layered neural network to a multiple layered neural network. In addition, many people in the field were using a learning function that was fundamentally flawed because it was not differentiable across the entire line. As a result, research and funding went drastically down.</blockquote><div class="attribution">&#8212; quote</div></div>
<div class="quoteblock"><blockquote>A quiet darkness fell across the neural networks, lasting many years. One might wonder what was happening in the USSR at this time, and the short answer is that cybernetics, as neural networks were still called in the USSR in this period, was considered a bourgeois pseudoscience.</blockquote><div class="attribution">&#8212; From logic</div></div></section>
<section id="_1972_prolog"><h2>1972, PROLOG</h2><div class="ulist"><ul><li><p>French scientist Alain Colmerauer invents the logic programming language</p></li></ul></div></section>
<section id="_1972_kohonen_and_anderson_developed_a_similar_network_independently_of_one_another"><h2>1972, Kohonen and Anderson developed a similar network independently of one another</h2><div class="paragraph"><p>"They both used matrix mathematics to describe their ideas but did not realize that what they were doing was creating an array of analog ADALINE circuits. The neurons are supposed to activate a set of outputs instead of just one."</p></div></section>
<section id="_1975_the_first_multilayered_network"><h2>1975 The first multilayered network</h2><div class="ulist"><ul><li><p>unsupervised network</p></li></ul></div></section>
<section id="_начало_70_х_mycin"><h2>начало 70-х MYCIN</h2><div class="ulist"><ul><li><p>Stanford</p></li><li><p>Lisp</p></li><li><p>разрабатывалась 5-6 лет</p></li><li><p>относительно простой алгоритм вывода (backward reasoning)</p></li><li><p>около 600 правил</p></li><li><p>программа задавала пользователю (врачу) длинный ряд простых «да/нет» или текстовых вопросов</p></li><li><p>система предоставляла список подозреваемых бактерий, отсортированный по вероятности, указывала доверительный интервал для вероятностей диагнозов и их обоснование</p></li><li><p>Research conducted at the Stanford Medical School found MYCIN received an acceptability rating of 65% on treatment plan from a panel of eight independent specialists, which was comparable to the 42.5% to 62.5% rating of five faculty members.</p></li></ul></div>
<div class="ulist"><div class="title">Проблемы</div><ul><li><p>этические вопросы</p></li><li><p>MYCIN была автономной системой, требующей от пользователя набора всей необходимой информации. Программа запускалась на сервере с разделением времени, доступному по раннему Интернету (ARPANet)</p></li><li><p>сеанс работы с MYCIN мог легко занять 30 минут и более</p></li><li><p>"knowledge acquisition bottleneck"&#8201;&#8212;&#8201;трудно "извлечь" знания из опыта людей-экспертов для формирования базы правил</p></li></ul></div></section>
<section id="_1982_hopfield_network"><h2>1982, Hopfield Network</h2><div class="ulist"><ul><li><p>a form of recurrent artificial neural network</p></li><li><p>Hopfield nets serve as content-addressable ("associative") memory systems with binary threshold nodes.</p></li></ul></div></section>
<section id="_1982_5_поколения_комьпютеров"><h2>1982 5 поколения комьпютеров</h2><div class="ulist"><ul><li><p>a joint US-Japan conference on Cooperative/Competitive Neural Networks.</p></li><li><p>Japan announced a new Fifth Generation effort on neural networks, and US papers generated worry that the US could be left behind in the field.</p></li><li><p>As a result, there was more funding and thus more research in the field.</p></li></ul></div></section>
<section id="_1986_back_propagation"><h2>1986 Back-propagation</h2><div class="quoteblock"><blockquote>In 1986, with multiple layered neural networks in the news, the problem was how to extend the Widrow-Hoff rule to multiple layers. Three independent groups of researchers, one of which included David Rumelhart, a former member of Stanford&#8217;s psychology department, came up with similar ideas which are now called back propagation networks because it distributes pattern recognition errors throughout the network. Hybrid networks used just two layers, these back-propagation networks use many. The result is that back-propagation networks are "slow learners," needing possibly thousands of iterations to learn.</blockquote><div class="attribution">&#8212; quote</div></div></section>
<section id="_1986_neurips"><h2>1986, NeurIPS</h2><div class="ulist"><ul><li><p>Conference on Neural Information Processing Systems</p></li><li><p>раньше звался NIPS</p></li><li><p>NeurIPS was designed as a complementary open interdisciplinary meeting for researchers exploring biological and artificial Neural Networks.</p></li></ul></div></section>
<section id="_ai_winter_2"><h2>AI Winter 2</h2><div class="paragraph"><p>Research that concentrates on developing neural networks is relatively slow. Due to the limitations of processors, neural networks take weeks to learn. Some companies are trying to create what is called a "silicon compiler" to generate a specific type of integrated circuit that is optimized for the application of neural networks. Digital, analog, and optical chips are the different types of chips being developed. One might immediately discount analog signals as a thing of the past. However neurons in the brain actually work more like analog signals than digital signals. While digital signals have two distinct states (1 or 0, on or off), analog signals vary between minimum and maximum values. It may be awhile, though, before optical chips can be used in commercial applications.</p></div></section></section>
<section id="_развитие"><h2>Развитие</h2><section id="_1979_neocognitron"><h2>1979 Neocognitron</h2><div class="videoblock stretch"><video src="images/Qil4kmvm2Sw" width="100%" height="100%" controls>Your browser does not support the video tag.</video></div></section>
<section id="_1979_aaai"><h2>1979, AAAI</h2><div class="ulist"><ul><li><p>American Association for Artificial Intelligence</p></li><li><p>теперь Association for the Advancement of Artificial Intelligence</p></li></ul></div>
<div class="paragraph"><p><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" class="bare">https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence</a></p></div></section>
<section id="_1986_rnn"><h2>1986, RNN</h2><div class="ulist"><ul><li><p>Jordan</p></li></ul></div></section>
<section id="_1987"><h2>1987,</h2><div class="literalblock"><div class="content"><pre>collapse of the LISP machine market</pre></div></div></section>
<section id="_1986_navlab_1"><h2>1986, Navlab 1</h2><div class="ulist"><ul><li><p>Carnegie Mellon University</p></li><li><p>Chevrolet panel van.</p></li><li><p>The van had 5 racks of computer hardware, including 3 Sun workstations, video hardware and GPS receiver, and a Warp supercomputer.</p></li><li><p>The vehicle suffered from software limitations and was not fully functional until the late 80s, when it achieved its top speed of 20 mph (32 km/h).</p></li></ul></div></section>
<section id="_1989_lenet"><h2>1989 LeNet</h2><div class="ulist"><ul><li><p>Lecun Net</p></li><li><p>Ян Лекун</p></li><li><p>60 тысяч параметров</p></li></ul></div>
<div class="imageblock" style=""><img src="https://miro.medium.com/max/800/0*MU7G1aH1jw-6eFiD.png" alt="0*MU7G1aH1jw 6eFiD"></div>
<div class="imageblock" style=""><img src="http://yann.lecun.com/exdb/lenet/gifs/asamples.gif" alt="asamples"></div></section>
<section id="_1990_srnn"><h2>1990, SRNN</h2><div class="ulist"><ul><li><p>the simple recurrent neural network (Elman network)</p></li><li><p>Elman Jeff (1948-2018)</p></li><li><p>"Finding structure in time"</p></li><li><p>The paper was ground-breaking for many cognitive scientists and psycholinguists, since it was the first to completely break away from a prior commitment to specific linguistic units (e.g. phonemes or words), and to explore the vision that these units might be emergent consequences of a learning process operating over the latent structure in the speech stream.</p></li></ul></div></section>
<section id="_1992_раии"><h2>1992, РАИИ</h2><div class="ulist"><ul><li><p>Российская ассоциация искусственного интеллекта</p></li></ul></div>
<div class="quoteblock"><blockquote>
К настоящему времени в ассоциацию входит 236 человек, из них 48 докторов и 69 кандидатов наук из 45 регионов России. Ассоциация провела шесть школ по проблемам искусственного интеллекта, участвовала в организации 14 симпозиумов, в том числе провела два международных симпозиума "Восток-Запад " (Москва 1993г., 1995г.), три международных симпозиума по технологии программирования (Переславль-Залесский 1994г., Болгария 1996г.), два международных семинара по прикладной семиотике (Будапешт 1996г., Смоленице 1997г.). Ассоциация участвовала в организации и проведении четырех международных конференций, в частности, Европейских конференций по искусственному интеллекту (Вена 1994г., Будапешт 1996г., Берлин 2000г.).
</blockquote><div class="attribution">&#8212; <a href="http://raai.org/about/about.shtml" class="bare">http://raai.org/about/about.shtml</a></div></div>
<div class="paragraph"><p>"В России Ассоциацией проведено тринадцать национальных конференций по искусственному интеллекту (Пеpеславль-Залесский 1988г., Минск 1990г., Тверь 1992г., Рыбинск 1994г., Казань 1996г., Пущино 1998г., Пеpеславль-Залесский 2000г., Коломна 2002г., Тверь 2004г., Обнинск 2006г., Дубна 2008г., Тверь 2010г., Белгород 2012г.)."</p></div>
<div class="paragraph"><p>&#8201;&#8212;&#8201;<a href="http://raai.org/about/about.shtml" class="bare">http://raai.org/about/about.shtml</a></p></div></section>
<section id="_1993_robocup"><h2>1993, RoboCup</h2><div class="ulist"><ul><li><p>Worldwide RoboCup initiative to build soccer-playing autonomous robots</p></li></ul></div></section>
<section id="_1995_svm"><h2>1995, SVM</h2><div class="paragraph"><p>This leads us to the 1990s and beyond. The early 1990s were largely uneventful,
as the general support of the AI community shifted towards support vector machines (SVM). These machine learning algorithms are mathematically well founded, as opposed to neural networks which were interesting from a philosophical standpoint,
and mainly developed by psychologists and cognitive scientists. To the larger AI community, which still had a lot of the GOFAI drive for mathematical precision,
they were uninteresting, and SVMs seemed to produce better results as well</p></div></section>
<section id="_1995_aima"><h2>1995, AIMA</h2><div class="ulist"><ul><li><p>Artificial Intelligence: A Modern Approach</p></li><li><p>Stuart J. Russell и Peter Norvig.</p></li><li><p>учебник</p></li></ul></div>
<div class="imageblock" style=""><img src="http://people.eecs.berkeley.edu/~russell/aima-cover-591KB.gif" alt="aima cover 591KB"></div></section>
<section id="_1997_video_rewrite"><h2>1997, Video Rewrite</h2><div class="paragraph"><p>-</p></div></section>
<section id="_1997_deep_blue"><h2>1997, Deep Blue</h2><div class="ulist"><ul><li><p>IBM’s chess computer Deep Blue defeats the chess world champion Gary Kasparov.</p></li></ul></div></section>
<section id="_1997_lstm"><h2>1997 LSTM</h2><div class="ulist"><ul><li><p>Sepp Hochreiter, Jürgen Schmidhuber.</p></li><li><p>популярная архитектура</p></li></ul></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/1024px-The_LSTM_cell.png" alt="1024px The LSTM cell"></div></section>
<section id="_1999_geforce_256"><h2>1999, GeForce 256</h2><div class="ulist"><ul><li><p>первый GeForce</p></li><li><p>"the world&#8217;s first 'GPU', or Graphics Processing Unit"</p></li><li><p>"a single-chip processor with integrated transform, lighting, triangle setup/clipping, and rendering engines that is capable of processing a minimum of 10 million polygons per second."</p></li></ul></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e1/VisionTek_GeForce_256.jpg/1280px-VisionTek_GeForce_256.jpg" alt="1280px VisionTek GeForce 256"></div></section>
<section id="_2001_neural_language_models"><h2>2001, Neural language models</h2><div class="imageblock" style=""><img src="http://ruder.io/content/images/2018/09/lm_bengio_2003.png" alt="lm bengio 2003"></div></section>
<section id="_2001_mit_opencourseware"><h2>2001, MIT OpenCourseWare</h2><div class="quoteblock"><blockquote>initiative of the Massachusetts Institute of Technology (MIT) to publish all of the educational materials from its undergraduate- and graduate-level courses online, freely and openly available to anyone, anywhere.</blockquote><div class="attribution">&#8212; <a href="https://en.wikipedia.org/wiki/MIT_OpenCourseWare" class="bare">https://en.wikipedia.org/wiki/MIT_OpenCourseWare</a></div></div></section>
<section id="_2004_darpa_grand_challenge"><h2>2004, DARPA Grand Challenge</h2><div class="ulist"><ul><li><p>Defense Advanced Research Projects Agency</p></li><li><p>ни одна машина не доехала</p></li></ul></div></section>
<section id="_2005_stanley"><h2>2005, Stanley</h2><div class="paragraph"><p>Stanley is an autonomous car created by Stanford University&#8217;s Stanford Racing Team in cooperation with the Volkswagen Electronics Research Laboratory (ERL). It won the 2005 DARPA Grand Challenge,[1] earning the Stanford Racing Team the 2 million dollar prize.</p></div>
<div class="imageblock" style=""><img src="https://www.wired.com/wp-content/uploads/archive/images/article/full/2007/10/darpa_urban_challenge_630px.jpg" alt="darpa urban challenge 630px"></div></section>
<section id="_2005_imagenet_challenge"><h2>2005, ImageNet Challenge</h2><div class="ulist"><ul><li><p>The ImageNet project is a large visual database designed for use in visual object recognition software research.</p></li><li><p>ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</p></li></ul></div></section>
<section id="_2006_dbn"><h2>2006 DBN</h2><div class="ulist"><ul><li><p>Deep Belief Network</p></li><li><p>Hinton, Osindero, Teh</p></li><li><p>significantly better results on the MNIST dataset</p></li><li><p>deep learning?</p></li></ul></div></section></section>
<section id="_deep_learning"><h2>Deep Learning</h2><section id="_2007_нти"><h2>2007, НТИ</h2><div class="ulist"><ul><li><p>Национа́льная технологи́ческая инициати́ва</p></li><li><p>государственная программа мер по поддержке развития в России перспективных отраслей, которые в течение следующих 20 лет могут стать основой мировой экономики</p></li></ul></div></section>
<section id="_список_центров_компетенций_нти"><h2>Список центров компетенций НТИ</h2><table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:33.3333%"><col style="width:33.3333%"><col style="width:33.3334%"></colgroup><thead><tr><th class="tableblock halign-left valign-top">Сквозная технология</th><th class="tableblock halign-left valign-top">Победитель конкурсного отбора</th><th class="tableblock halign-left valign-top">Наименование Центра</th></tr><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Искусственный интеллект</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">МФТИ</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Центр Национальной технологической инициативы по направлению «Искусственный интеллект»</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Технологии хранения и анализа больших данных</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">МГУ имени М.В. Ломоносова</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Центр технологий хранения и анализа больших данных</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Технологии машинного обучения и когнитивные технологии</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">ИТМО</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Национальный центр когнитивных разработок</p></td></tr></table>
<div class="paragraph"><p><a href="https://ru.wikipedia.org/wiki/Национальная_технологическая_инициатива" class="bare">https://ru.wikipedia.org/wiki/Национальная_технологическая_инициатива</a></p></div></section>
<section id="_2007_theano"><h2>2007, Theano</h2><div class="ulist"><ul><li><p>Montreal Institute for Learning Algorithms</p></li></ul></div></section>
<section id="_2008_multi_task_learning_для_нейросетей"><h2>2008, Multi-task learning для нейросетей</h2><div class="ulist"><ul><li><p>A unified architecture for natural language processing: Deep neural networks with multitask learning</p></li><li><p>the look-up tables (or word embedding matrices) are shared between two models trained on different tasks</p></li><li><p>It won the test-of-time award at ICML 2018</p></li><li><p>Multi-task learning is now used across a wide range of NLP tasks and leveraging existing or "artificial" tasks has become a useful tool in the NLP repertoire.</p></li></ul></div>
<div class="imageblock" style=""><img src="http://ruder.io/content/images/2018/09/collobert_icml2008.png" alt="collobert icml2008"></div></section>
<section id="_2009_aima_3rd_edition"><h2>2009, AIMA 3rd edition</h2><div class="ulist"><ul><li><p>"the most popular artificial intelligence textbook in the world"</p></li><li><p>7 вузов России: Altai State Univ., Kazan State Univ., Lobachevsky State Univ. of Nizhni Novgorod , Moscow State Industrial Univ. , Moscow State Univ., Statistics and Informatics, St. Petersburg State Univ. of Info. Tech., V.A. Steklov Inst. of Math, Russian Academy of Sciences, St. Petersburg</p></li><li><p>Part I: Artificial Intelligence - Sets the stage for the following sections by viewing AI systems as intelligent agents that can decide what actions to take and when to take them.</p></li><li><p>Part II: Problem-solving - Focuses on methods for deciding what action to take when needing to think several steps ahead such as playing a game of chess.</p></li><li><p>Part III: Knowledge, reasoning, and planning - Discusses ways to represent knowledge about the intelligent agents' environment and how to reason logically with that knowledge.</p></li><li><p>Part IV: Uncertain knowledge and reasoning - This section is analogous to Parts III, but deals with reasoning and decision-making in the presence of uncertainty in the environment.</p></li><li><p>Part V: Learning - Describes ways for generating knowledge required by the decision-making components and introduces a new component: the artificial neural network</p></li><li><p>Part VI: Communicating, perceiving, and acting - Concentrates on ways an intelligent agent can perceive its environment whether by touch	or vision.</p></li><li><p>Part VII: Conclusions	- Considers the past and future of AI by discussing what AI really is and why it has succeeded to some degree. Also discusses the views of those philosophers who believe that AI can never succeed.</p></li></ul></div></section>
<section id="_2009_the_unreasonable_effectiveness_of_data"><h2>2009, The Unreasonable Effectiveness of Data</h2><div class="ulist"><ul><li><p>Fernando Pereira, Peter Norvig, Alon Halevy</p></li><li><p>Google</p></li></ul></div>
<div class="paragraph"><p><a href="https://ai.google/research/pubs/pub35179/" class="bare">https://ai.google/research/pubs/pub35179/</a></p></div>
<div class="imageblock" style=""><img src="images/2019-09-13-14-25-15.png" alt="2019 09 13 14 25 15"></div>
<div class="paragraph"><p><a href="http://aima.cs.berkeley.edu/adoptions.html" class="bare">http://aima.cs.berkeley.edu/adoptions.html</a></p></div></section>
<section id="_2009_google_self_driving"><h2>2009, Google self-driving</h2><div class="ulist"><ul><li><p>First Google self-driving car drives on the California freeway.</p></li></ul></div></section>
<section id="_2011_google_brain"><h2>2011, Google Brain</h2><div class="ulist"><ul><li><p>deep learning artificial intelligence research team at Google.</p></li><li><p>part-time research collaboration between Google Fellow Jeff Dean, Google Researcher Greg Corrado, and Stanford University professor Andrew Ng</p></li><li><p>2013 наняли Хинтона</p></li></ul></div></section>
<section id="_2011_ml_class_com"><h2>2011, ml-class.com</h2><div class="ulist"><ul><li><p>Andrew Ng, Daphne Koller</p></li><li><p>100,000 students</p></li><li><p>из него появились Coursera</p></li></ul></div>
<div class="imageblock" style=""><img src="images/2019-09-13-15-45-27.png" alt="2019 09 13 15 45 27"></div></section>
<section id="_2011_ai_class_com"><h2>2011, ai-class.com</h2><div class="ulist"><ul><li><p>Introduction to Artificial Intelligence</p></li><li><p>Stanford</p></li><li><p>Peter Norvig and Sebastian Thrun</p></li><li><p>160,000 students signed up, from more than 190 countries, with a median age of around 30</p></li><li><p>из него появилось Udacity</p></li></ul></div>
<div class="imageblock" style=""><img src="https://i.guim.co.uk/img/static/sys-images/Admin/BkFill/Default_image_group/2012/2/2/1328202623513/Computer-scientists-Peter-007.jpg?width=620&quality=85&auto=format&fit=max&s=db6c7fa7f8fa3882314b28320fc1c54e" alt="Computer scientists Peter 007"></div></section>
<section id="_2011_watson"><h2>2011, Watson</h2><div class="paragraph"><p>IBM’s “Watson” beats two human champions on the television game show “Jeopardy!”</p></div>
<div class="imageblock" style=""><img src="https://cbsnews3.cbsistatic.com/hub/i/r/2011/02/17/c8a7ec73-a642-11e2-a3f0-029118418759/thumbnail/620x465/a106dd6de4b9fc18a8e903f1bd30acc6/AP110114053298.jpg" alt="AP110114053298"></div></section>
<section id="_2012_alexnet"><h2>2012 AlexNet</h2><div class="ulist"><ul><li><p>Alex Krizhevsky,  Ilya Sutskever, Geoffrey Hinton</p></li><li><p>GPU</p></li></ul></div>
<div class="imageblock" style=""><img src="https://miro.medium.com/max/932/1*wzflNwJw9QkjWWvTosXhNw.png" alt="1*wzflNwJw9QkjWWvTosXhNw"></div></section>
<section id="_2012_alexnet_imagenet"><h2>2012, AlexNet ImageNet</h2><div class="imageblock" style=""><img src="http://www.programmersought.com/images/528/5d6b1e9d70bf0d3b5212353ebd239698.JPEG" alt="5d6b1e9d70bf0d3b5212353ebd239698"></div></section>
<section id="_2013_word2vec"><h2>2013, Word2Vec</h2><div class="ulist"><ul><li><p>Томаш Миколов (1982-), Google</p></li><li><p>дистрибутивная семантика</p></li><li><p>быстрый алгоритм</p></li><li><p>удачный выбор дефолтный гиперпараметров</p></li><li><p>"семантическая арифметика"</p></li></ul></div>
<div class="imageblock" style=""><img src="http://jalammar.github.io/images/word2vec/king-analogy-viz.png" alt="king analogy viz"></div></section>
<section id="_2013_ai2"><h2>2013, AI2</h2><div class="ulist"><ul><li><p>Allen Institute for Artificial Intelligence</p></li><li><p>founded by late Microsoft co-founder Paul Allen.</p></li></ul></div>
<div class="paragraph"><p>проекты:
- Semantic Scholar
- AllenNLP
- MOSAIC: The Mosaic project is focused on defining and building common sense knowledge and reasoning for AI systems.</p></div></section>
<section id="_2014_nvidia_jetson"><h2>2014, Nvidia Jetson</h2><div class="ulist"><ul><li><p>Nvidia Jetson TK1</p></li></ul></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Nvidia_Jetson_TK1_-_Full_Board_%2814672953894%29.png/330px-Nvidia_Jetson_TK1_-_Full_Board_%2814672953894%29.png" alt="330px Nvidia Jetson TK1   Full Board %2814672953894%29"></div></section>
<section id="_2014_gan"><h2>2014, GAN</h2><div class="ulist"><ul><li><p>Generative Adversarial Networks</p></li><li><p>Goodfellow, Ian; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua</p></li></ul></div>
<div class="imageblock" style=""><img src="https://skymind.ai/images/wiki/GANs.png" alt="GANs"></div></section>
<section id="_2015_the_unreasonable_effectiveness_of_recurrent_neural_networks"><h2>2015, The Unreasonable Effectiveness of Recurrent Neural Networks</h2><div class="ulist"><ul><li><p>Andrey Karpathy</p></li><li><p>char-based</p></li><li><p>генерация Шекспира, кода и т.п.</p></li><li><p>интерпретация</p></li></ul></div>
<div class="imageblock" style=""><img src="http://karpathy.github.io/assets/rnn/pane1.png" alt="pane1"></div>
<div class="paragraph"><p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="bare">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p></div></section>
<section id="_2015_resnet"><h2>2015 ResNet</h2><div class="ulist"><ul><li><p>Residual Neural Network (ResNet)</p></li><li><p>ILSVRC 2015</p></li><li><p>Kaiming He и компания</p></li><li><p>“skip connections” and features heavy batch normalization.</p></li></ul></div>
<div class="imageblock" style=""><img src="https://www.pfmjournal.org/upload/thumbnails/pfm-2018-00030f1.jpg" alt="pfm 2018 00030f1"></div></section>
<section id="_2015_keras"><h2>2015 Keras</h2><div class="ulist"><ul><li><p>François Chollet</p></li><li><p>Python</p></li><li><p>2017 часть Tensorflow</p></li></ul></div></section>
<section id="_2015_заметный_рост_nips"><h2>2015, заметный рост NIPS</h2><div class="imageblock" style=""><img src="https://signalprocessingsociety.org/uploads/images/SLTC-Newsletter/NipsGrowth.png" alt="NipsGrowth"></div>
<div class="paragraph"><p><a href="https://signalprocessingsociety.org/get-involved/speech-and-language-processing/newsletter/brief-review-nips-2015" class="bare">https://signalprocessingsociety.org/get-involved/speech-and-language-processing/newsletter/brief-review-nips-2015</a></p></div>
<div class="imageblock" style=""><img src="https://pbs.twimg.com/media/Cv-qC4TVIAEZ9Tp.jpg" alt="Cv qC4TVIAEZ9Tp"></div></section>
<section id="_2015_tensorflow"><h2>2015, Tensorflow</h2></section>
<section id="_2015_openai"><h2>2015, OpenAI</h2><div class="ulist"><ul><li><p>основатели Elon Musk и Sam Altman</p></li><li><p>1 млрд $</p></li><li><p>в 2019 году Майкрософт еще 1 млрд $</p></li></ul></div></section>
<section id="_2015_deep_learning"><h2>2015, Deep learning</h2><div class="ulist"><ul><li><p>Йошуа Бенджио, Джеффри Хинтон и Ян Лекун</p></li><li><p>статья в Nature
image::2019-09-13-13-49-53.png[]</p></li></ul></div>
<div class="paragraph"><p><a href="https://www.nature.com/articles/nature14539" class="bare">https://www.nature.com/articles/nature14539</a></p></div></section>
<section id="_2016_pytorch"><h2>2016 PyTorch</h2><div class="ulist"><ul><li><p>Facebook</p></li><li><p>на базе Torch</p></li></ul></div></section>
<section id="_2016_openai_gym"><h2>2016, OpenAI Gym</h2></section>
<section id="_2016_tpu"><h2>2016, TPU</h2><div class="ulist"><ul><li><p>tensor processing unit</p></li><li><p>AI accelerator application-specific integrated circuit</p></li><li><p>Google</p></li><li><p>2017 версия 2</p></li><li><p>2018 версия 3</p></li></ul></div>
<div class="imageblock" style=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/be/Tensor_Processing_Unit_3.0.jpg/330px-Tensor_Processing_Unit_3.0.jpg" alt="330px Tensor Processing Unit 3.0"></div></section>
<section id="_2016_alphago"><h2>2016, AlphaGo</h2><div class="quoteblock"><blockquote>The Go program AlphaGo by Google DeepMind beats the European
champion 5:0 in January and Korean Lee Sedol, one of the world’s best Go players,
4:1 in March. Deep learning techniques applied to pattern recognition, as well as
reinforcement learning and Monte Carlo tree search lead to this success.</blockquote><div class="attribution">&#8212; quote</div></div></section>
<section id="_2016_эмпиризм_в_deep_learning"><h2>2016, Эмпиризм в deep learning?</h2><div class="quoteblock"><blockquote>By working strictly on methods that you can fully analyze theoretically, you confine yourself to using excessively naive methods.
Physicists don’t work like that. They don’t get to choose the complexity of the systems they study: the physical world is what it is. To them, complex systems are more interesting. For example, a lot of interesting mathematics and theoretical physics methods were developed in the context of studying spin glasses and other “disordered” systems. Physicists couldn’t simply choose to not study these systems because they were too complicated.</blockquote><div class="attribution">&#8212; <a href="https://www.quora.com/Are-you-concerned-about-research-in-NN-and-Deep-Learning-being-too-much-results-driven-without-backing-with-a-strong-theoretical-explaination/answer/Yann-LeCun" class="bare">https://www.quora.com/Are-you-concerned-about-research-in-NN-and-Deep-Learning-being-too-much-results-driven-without-backing-with-a-strong-theoretical-explaination/answer/Yann-LeCun</a></div></div>
<div class="quoteblock"><blockquote>I do think that there is a need for better theoretical understanding of deep learning.</blockquote><div class="attribution">&#8212; <a href="https://www.quora.com/Are-you-concerned-about-research-in-NN-and-Deep-Learning-being-too-much-results-driven-without-backing-with-a-strong-theoretical-explaination/answer/Yann-LeCun" class="bare">https://www.quora.com/Are-you-concerned-about-research-in-NN-and-Deep-Learning-being-too-much-results-driven-without-backing-with-a-strong-theoretical-explaination/answer/Yann-LeCun</a></div></div>
<div class="quoteblock"><blockquote>How important is the interpretability of your taxi driver?</blockquote><div class="attribution">&#8212; <a href="https://www.quora.com/How-important-is-interpretability-for-a-model-in-Machine-Learning/answer/Yann-LeCun" class="bare">https://www.quora.com/How-important-is-interpretability-for-a-model-in-Machine-Learning/answer/Yann-LeCun</a></div></div>
<div class="quoteblock"><blockquote>
Contrary to what some people think, I believe that we have already a good basic understanding of fundamentals about why deep learning works, e.g.,
But of course, much more theory is needed! Very often we are in a situation where we do not understand the results of an experiment, for example.</blockquote><div class="attribution">&#8212; <a href="https://www.quora.com/How-far-along-are-we-in-the-understanding-of-why-deep-learning-works/answer/Yoshua-Bengio" class="bare">https://www.quora.com/How-far-along-are-we-in-the-understanding-of-why-deep-learning-works/answer/Yoshua-Bengio</a></div></div></section>
<section id="_2016_partnership_on_ai"><h2>2016, Partnership on AI</h2><div class="ulist"><ul><li><p>Partnership on Artificial Intelligence to Benefit People and Society</p></li><li><p>индустриальный консорциум</p></li><li><p>"establishing best practices for artificial intelligence systems and to educate the public about AI"</p></li><li><p>Amazon, Facebook, Google, DeepMind, Microsoft, and IBM</p></li><li><p>Apple присоединились в 2017</p></li></ul></div></section>
<section id="_2017_onnx"><h2>2017, ONNX</h2><div class="ulist"><ul><li><p>Open Neural Network Exchange</p></li><li><p>Facebook и Microsoft</p></li><li><p>затем IBM, Huawei, Intel, AMD, ARM and Qualcomm</p></li></ul></div></section>
<section id="_2017_ipavlov"><h2>2017, iPavlov</h2><div class="quoteblock"><blockquote>Согласно паспорту проекта, с 2017 по 2020 год iPavlov должен получить 505,6 млн рублей, в том числе 350,3 млн в качестве субсидий из федерального бюджета по линии Национальной технологической инициативы и 155,3 млн от частных инвесторов.</blockquote><div class="attribution">&#8212; <a href="https://vc.ru/finance/22554-sber-ipavlov" class="bare">https://vc.ru/finance/22554-sber-ipavlov</a></div></div>
<div class="quoteblock"><blockquote>«МФТИ — один из наших ключевых партнёров в сфере образования и создания инновационных технологий, — отметил Герман Греф. — Результатом проекта iPavlov станут новые бизнес-приложения, которые будут интегрированы в работу Сбербанка и выведут обслуживание наших клиентов на новый уровень. Кроме того, в ходе проекта Сбербанк и МФТИ проведут масштабные междисциплинарные исследования, которые помогут создать экосистему мирового уровня в области машинного обучения и искусственного интеллекта».</blockquote><div class="attribution">&#8212; <a href="http://rusneuro.net/novosti/v-mfti-startuet-proekt-ipavlov-dorognoi-karty-neironet-po-razrabotke-novyh-tehnologii-masinnogo-intellekta" class="bare">http://rusneuro.net/novosti/v-mfti-startuet-proekt-ipavlov-dorognoi-karty-neironet-po-razrabotke-novyh-tehnologii-masinnogo-intellekta</a></div></div></section>
<section id="_2018_edge_tpu"><h2>2018, Edge TPU</h2><div class="imageblock" style=""><img src="https://lh3.googleusercontent.com/hau9Cug0FnHUkeuyjdQK9fN4vvsIreHfwSpLTpxnfWWQkafxMfVTE5UN81bdzn3sepLxmZiPJKCzhbaQGZd5o8oV3srlyEf4Ab9XVg=w1000-rw" alt="hau9Cug0FnHUkeuyjdQK9fN4vvsIreHfwSpLTpxnfWWQkafxMfVTE5UN81bdzn3sepLxmZiPJKCzhbaQGZd5o8oV3srlyEf4Ab9XVg=w1000 rw"></div></section>
<section id="_2018"><h2>2018,</h2></section>
<section id="_2019_openai_five"><h2>2019, OpenAI Five</h2><div class="ulist"><ul><li><p>Dota</p></li><li><p>OpenAI Five defeated OG, the reigning world champions of the game at the time, 2:0 in a live exhibition match in San Francisco</p></li><li><p>OpenAI only learned a heavily simplified version of Dota, including only 17 out of over 100 heroes and excluding certain items as well as game mechanics.</p></li></ul></div></section>
<section id="_2019_gpt_2"><h2>2019, GPT-2</h2><div class="ulist"><ul><li><p>neural fake news</p></li><li><p>десятки тысяч $</p></li></ul></div></section>
<section id="_2019_премия_тьюринга"><h2>2019, Премия Тьюринга</h2><div class="ulist"><ul><li><p>Йошуа Бенджио, Джеффри Хинтон и Ян Лекун</p></li></ul></div>
<div class="paragraph"><p><a href="https://amturing.acm.org" class="bare">https://amturing.acm.org</a></p></div></section>
<section id="_2019_национальной_стратегии_развития_технологий_ии"><h2>2019, Национальной стратегии развития технологий ИИ</h2><div class="quoteblock"><blockquote>По словам главы государства, уже через пять лет мировой рынок продуктов с использованием искусственного интеллекта вырастет почти в 17 раз и составит порядка 1,5 трлн рублей.</blockquote><div class="attribution">&#8212; <a href="https://iz.ru/886709/2019-06-07/v-rossii-poiavitsia-natcstrategiia-v-oblasti-iskusstvennogo-intellekta" class="bare">https://iz.ru/886709/2019-06-07/v-rossii-poiavitsia-natcstrategiia-v-oblasti-iskusstvennogo-intellekta</a></div></div></section>
<section id="_2019_human_compatible_artificial_intelligence_and_the_problem_of_control"><h2>2019, Human Compatible: Artificial Intelligence and the Problem of Control</h2><div class="imageblock" style=""><img src="https://images-na.ssl-images-amazon.com/images/I/41r9M-CBgrL._SX329_BO1,204,203,200_.jpg" alt="41r9M CBgrL. SX329 BO1,204,203,200 "></div></section>
<section id="_timeline_1"><h2>Timeline 1</h2><div class="imageblock" style=""><img src="images/histoty-fukushima.png" alt="histoty fukushima"></div></section></section>
<section id="_sota"><h2>SOTA</h2><section id="_poc"><h2>POC</h2><div class="paragraph"><p><a href="https://paperswithcode.com" class="bare">https://paperswithcode.com</a></p></div></section>
<section id="_eff"><h2>EFF</h2><div class="paragraph"><p><a href="https://www.eff.org/ai/metrics" class="bare">https://www.eff.org/ai/metrics</a></p></div></section>
<section id="_rusbase"><h2>Rusbase</h2><div class="paragraph"><p><a href="https://rb.ru/ai/" class="bare">https://rb.ru/ai/</a></p></div></section></section>
<section id="_курс"><h2>Курс</h2><section id="_домашки"><h2>Домашки</h2><div class="ulist"><ul><li><p>эссе</p></li><li><p>несколько домашних заданий</p></li><li><p>проект на 1-2 человек</p></li><li><p>рассказ о пейпере</p></li><li><p>защита</p></li></ul></div></section>
<section id="_темы"><h2>Темы</h2><div class="ulist"><ul><li><p>логистическая регрессия</p></li><li><p>pytorch</p></li><li><p>основы нейросетей</p></li><li><p>компьютерное зрение</p></li><li><p>компьютерная лингвистика</p></li><li><p>работа со звуком</p></li><li><p>генеративные модели</p></li><li><p>multi-task learning</p></li><li><p>high-level задачи</p></li></ul></div></section></section></div></div><script src="reveal.js/lib/js/head.min.js"></script><script src="reveal.js/js/reveal.js"></script><script>Array.prototype.slice.call(document.querySelectorAll('.slides section')).forEach(function(slide) {
  if (slide.getAttribute('data-background-color')) return;
  // user needs to explicitly say he wants CSS color to override otherwise we might break custom css or theme (#226)
  if (!(slide.classList.contains('canvas') || slide.classList.contains('background'))) return;
  var bgColor = getComputedStyle(slide).backgroundColor;
  if (bgColor !== 'rgba(0, 0, 0, 0)' && bgColor !== 'transparent') {
    slide.setAttribute('data-background-color', bgColor);
    slide.style.backgroundColor = 'transparent';
  }
})

// See https://github.com/hakimel/reveal.js#configuration for a full list of configuration options
Reveal.initialize({
  // Display presentation control arrows
  controls: true,
  // Help the user learn the controls by providing hints, for example by
  // bouncing the down arrow when they first encounter a vertical slide
  controlsTutorial: true,
  // Determines where controls appear, "edges" or "bottom-right"
  controlsLayout: 'bottom-right',
  // Visibility rule for backwards navigation arrows; "faded", "hidden"
  // or "visible"
  controlsBackArrows: 'faded',
  // Display a presentation progress bar
  progress: true,
  // Display the page number of the current slide
  slideNumber: 'true',
  // Control which views the slide number displays on
  showSlideNumber: 'all',
  // Push each slide change to the browser history
  history: true,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Vertical centering of slides
  center: false,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // Randomizes the order of slides each time the presentation loads
  shuffle: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags whether to include the current fragment in the URL,
  // so that reloading brings you to the same fragment position
  fragmentInURL: false,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Flags if we should show a help overlay when the questionmark
  // key is pressed
  help: true,
  // Flags if speaker notes should be visible to all viewers
  showNotes: false,
  // Global override for autolaying embedded media (video/audio/iframe)
  // - null: Media will only autoplay if data-autoplay is present
  // - true: All media will autoplay, regardless of individual setting
  // - false: No media will autoplay, regardless of individual setting
  autoPlayMedia: null,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Use this method for navigation when auto-sliding
  autoSlideMethod: Reveal.navigateNext,
  // Specify the average time in seconds that you think you will spend
  // presenting each slide. This is used to show a pacing timer in the
  // speaker view
  defaultTiming: 120,
  // Enable slide navigation via mouse wheel
  mouseWheel: false,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  // Add `data-preview-link` and `data-preview-link="false"` to customise each link
  // individually
  previewLinks: false,
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: 'slide',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'fade',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',
  // Number of pixels to move the parallax background per slide
  // - Calculated automatically unless specified
  // - Set to 0 to disable movement along an axis
  parallaxBackgroundHorizontal: null,
  parallaxBackgroundVertical: null,
  // The display mode that will be used to show slides
  display: 'block',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 1600,
  height: 1200,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.5,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
      
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
      { src: 'reveal.js/plugin/notes/notes.js', async: true },
      
      
      
      { src: 'revealjs-plugins/reveal.js-menu/menu.js' },
{ src: 'revealjs-plugins/chalkboard/chalkboard.js' },
{ src: 'revealjs-plugins/reveal-code-focus/reveal-code-focus.js' }

  ],

  menu: {
	side: 'right',
	openSlideNumber: true,
	numbers: true,
},
keyboard: {
	    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle notes canvas when 'c' is pressed
	    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
	    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
	     8: function() { RevealChalkboard.reset() },	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
		68: function() { RevealChalkboard.download() },	// downlad recorded chalkboard drawing when 'd' is pressed,
		70: function() { RevealChalkboard.colorNext() },	// cycle colors forward when 'f' is pressed
	    71: function() { RevealChalkboard.colorPrev() },	// cycle colors backward when 'g' is pressed

},
chalkboard: { 
	toggleChalkboardButton: { left: "60px", bottom: "30px", top: "auto", right: "auto" },
	toggleNotesButton: { left: "90px", bottom: "30px", top: "auto", right: "auto" },
	readOnly: false,
	theme: 'whiteboard'
}


});</script></body></html>